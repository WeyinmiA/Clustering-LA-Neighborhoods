{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting and Clustering Neighborhoods in Los Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to survey and explore neighborhoods in Los Angeles (LA), California. LA is a big place, so to narrow down the scope of this investigation a bit, let's only consider zip codes that have a [sectional center facility](https://en.wikipedia.org/wiki/ZIP_Code#Structure_and_allocation) of 900 i.e. zip codes starting with 900!\n",
    "\n",
    "After the data has been loaded and preprocessed, a call to the Foursquare API will be made to explore and segment the neighborhoods in LA. Foursquare's **explore** function will be utilized to get the most common venue categories in each neighborhood, and then this feature will be used to group the neighborhoods into clusters. The clustering algorithm of choice to complete this task will be the *k*-means. Finally, the Folium library will be used to visualize the neighborhoods and their emerging clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. **Loading and Preprocessing Datasets**\n",
    "\n",
    "2. **Exploring the neighborhoods in LA**\n",
    "\n",
    "3. **Analyzing the neighborhoods in LA**\n",
    "\n",
    "4. **Clustering the neighborhoods in LA**\n",
    "\n",
    "5. **Examining the clusters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully installed!\n"
     ]
    }
   ],
   "source": [
    "import requests # library to handle requests\n",
    "#!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup # library for web scraping\n",
    "import pandas as pd # library to process data as dataframes\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium # map rendering library\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "print(\"Libraries successfully installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preprocessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STR_NM</th>\n",
       "      <th>STR_SFX_CD</th>\n",
       "      <th>ZIP_CD</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IVAR</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90028</td>\n",
       "      <td>34.10369</td>\n",
       "      <td>-118.32831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6TH</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90291</td>\n",
       "      <td>33.99944</td>\n",
       "      <td>-118.47219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BELFORD</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90045</td>\n",
       "      <td>33.95675</td>\n",
       "      <td>-118.38381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAUBERT</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90027</td>\n",
       "      <td>34.09890</td>\n",
       "      <td>-118.29008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BELFORD</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90045</td>\n",
       "      <td>33.95675</td>\n",
       "      <td>-118.38381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHELSEA</td>\n",
       "      <td>LANE</td>\n",
       "      <td>91304</td>\n",
       "      <td>34.22081</td>\n",
       "      <td>-118.60988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHEREMOYA</td>\n",
       "      <td>AVE</td>\n",
       "      <td>90068</td>\n",
       "      <td>34.10839</td>\n",
       "      <td>-118.32073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CRESTA</td>\n",
       "      <td>DR</td>\n",
       "      <td>90035</td>\n",
       "      <td>34.04616</td>\n",
       "      <td>-118.38767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FORTUNA</td>\n",
       "      <td>ST</td>\n",
       "      <td>90011</td>\n",
       "      <td>33.99288</td>\n",
       "      <td>-118.24543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YOLANDA</td>\n",
       "      <td>AVE</td>\n",
       "      <td>91356</td>\n",
       "      <td>34.17615</td>\n",
       "      <td>-118.54091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      STR_NM STR_SFX_CD  ZIP_CD       LAT        LON\n",
       "0       IVAR        AVE   90028  34.10369 -118.32831\n",
       "1        6TH        AVE   90291  33.99944 -118.47219\n",
       "2    BELFORD        AVE   90045  33.95675 -118.38381\n",
       "3    MAUBERT        AVE   90027  34.09890 -118.29008\n",
       "4    BELFORD        AVE   90045  33.95675 -118.38381\n",
       "5    CHELSEA       LANE   91304  34.22081 -118.60988\n",
       "6  CHEREMOYA        AVE   90068  34.10839 -118.32073\n",
       "7     CRESTA         DR   90035  34.04616 -118.38767\n",
       "8    FORTUNA         ST   90011  33.99288 -118.24543\n",
       "9    YOLANDA        AVE   91356  34.17615 -118.54091"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_data = pd.read_csv('Addresses_in_the_City_of_LA.csv')\n",
    "la_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004342, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the zip codes that start with 900, so we will need to filter our dataframe based on the 'ZIP_CD' column. First we will check what the type of the values in ZIP_CD. If it is of type int **we will convert it to a string object in order to take advantage of the readily available string methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_data.ZIP_CD.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected it is of type int, so lets now convert it to a string object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.ZIP_CD = la_data.ZIP_CD.astype('str')\n",
    "la_data.ZIP_CD.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data = la_data[la_data['ZIP_CD'].str.startswith('900')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move forward, let's clean our dataset a bit and combine the STR_NM and STR_SFX_CD columns to get a better representation of the street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_street = la_data.STR_NM + ' ' + la_data.STR_SFX_CD\n",
    "la_street.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add our improved street column back in and remove the old ones.\n",
    "la_data.insert(2,'STREET',la_street)\n",
    "la_data.drop(['STR_NM','STR_SFX_CD'], axis=1, inplace = True)\n",
    "la_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of data left, so let's look at our unique zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.ZIP_CD.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count them.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(la_data.ZIP_CD.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 63 unique zip codes. Let's look at the unique street names below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.STREET.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(la_data.STREET.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know that our df which has about **560K rows, is distributed between 63 unique zip codes and 4518 unique streets.**\n",
    "\n",
    "A problem with the dataset above is that it does not contain the name of the neighborhoods the streets are located in. To fix this, I found a [website](https://www.unitedstateszipcodes.org/) that could match a given zip code to its neighborhood. Instead of manually looking these up and noting down the conversions, let's try to automate this boring task by searching and scraping the data from the website using code. The programming tools of choice will be the Python libraries Requests and BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a get request to get the webpage\n",
    "zip_response = requests.get(\"https://www.unitedstateszipcodes.org/90007/\")\n",
    "zip_content = zip_response.content\n",
    "# Initialize the parser, and pass in the content we grabbed earlier.\n",
    "zip_parser = BeautifulSoup(zip_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selector below will target tr elements\n",
    "# We will then continue to extract the data we need.\n",
    "table = zip_parser.select('.table tbody tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a Web server receives an API request, it always returns a status code to provide information about what happened with the request. Let's now view the status code to confirm if the request was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resource you're trying to access is forbidden; you don't have the right permissions to see it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A status code of 403 means the resource we're trying to access is forbidden! This means that we don't have the right permissions to see it! As such, web scraping can't be used here. Luckily, there aren't so many zip codes to consider, so a manual approach will be utilized to get the neighborhoods that match each zip code!** \n",
    "\n",
    "To achieve this we will create a dictionary that maps each zip code to a neighborhood. \n",
    "Key-->zip code. Value--> Neighborhood. Let's have a quick look at the different zip codes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.ZIP_CD.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the zip codes are in a random order, but for the purpose of assigning each a neighborhood, the process might be simpler if the zip codes are arranged in a logical order. So let's sort the dataframe by the zip codes (in ascending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.sort_values(by = 'ZIP_CD', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our unique zip codes again.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.ZIP_CD.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the unique zip codes are now ordered logically. Let's convert this into a list of zip codes then create a list of neighborhoods too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_list = list(la_data.ZIP_CD.unique())\n",
    "zip_list[:5] # inspect first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_list = ['Los Angeles','Watts, Southeast Los Angeles','Southeast Los Angeles','Koreatown','Koreatown',\n",
    "              'Pico-Union, Koreatown','Los Angeles','Crenshaw','Koreatown','Southeast Los Angeles','Chinatown',\n",
    "              'Wholesale District, Downtown Los Angeles','Gallery Row, Fashion District, Downtown Los Angeles',\n",
    "              'South park, Downtown Los Angeles', 'West Adams',np.nan,np.nan,'Mid-Wilshire','Koreatown',\n",
    "              'Wholesale District, Downtown Los Angeles','Boyle Heights','Westwood','West Los Angeles',\n",
    "              'Echo Park','Griffith Park',np.nan,'East Hollywood','Lincoln Heights, Northeast Los Angeles',\n",
    "              'El Sereno, Northeast Los Angeles','Arroyo Seco, Boyle Heights','Palms','West Los Angeles',\n",
    "              'Mid-Wilshire',np.nan,np.nan,'Atwater Village, Northeast Los Angeles',\n",
    "              'Eagle Rock, Northeast Los Angeles','Highland Park, Northeast Los Angeles','Los Angeles',\n",
    "              'Los Angeles','Playa Del Rey','Hollywood Hills',np.nan,'Mid-Wilshire','Brentwood','Los Angeles',\n",
    "              np.nan,np.nan,'Los Angeles','Los Angeles',np.nan,'East Los Angeles',\n",
    "              'Century City, West Los Angeles','Mount Washington, Glassell Park, Northeast Los Angeles',\n",
    "              'Mar Vista','Century City, West Los Angeles','Hollywood Hills','Beverly Crest',\n",
    "              'Financial District, Downtown Los Angeles','Brentwood','Bel Air','Los Angeles','Playa Del Rey']\n",
    "\n",
    "#Fun facts: USC is in the wide area of 90007 and the USC Marshall School of Business is in 90089! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the size of the neighborhood list to confirm that it has 63 values to match the number of zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neigh_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay good! Just as we expected. Let's examine our dataframe one more time before updating it with the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was established earlier that we have **63 unique zip codes and 4518 unique streets**. However if you look at the dataframe, a single zip code may appear multiple times. Let's quickly examine the top 5 occurring zip codes to illustrate this point further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.ZIP_CD.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the top occurring zip code is 90011 which occurs almost 25K times! Considering there are only 3942 unique streets in this dataframe, that might mean many of the rows have similar values. Let's explore this further by seeing how many unique streets are present for the zip code 90011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_freq_zip_subset = la_data[la_data['ZIP_CD']=='90011'] # create a df that is a subset of the 90011 zip code\n",
    "top_freq_zip_subset.head() # check new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_freq_zip_subset.index) # confirm it has 24839 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's find out how many unique streets there are.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_freq_zip_subset.STREET.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 unique streets! There are 90 unique streets for almost 25,000 occurrences of the 90011 zip code. To refine the dataset, such as to present neighborhood data as opposed to just street data, **we will choose the rows of the highest occurring streets per neighborhood.** \n",
    "\n",
    "This will help in the later stages of analysis, as we will want to use the latitude and longitude values for our records to visualize our neighborhoods on a map of LA. The problem with the way the dataframe is set up right now is that for the same neighborhood and zip code, we are getting multiple latitude and longitude values because of the different streets in a neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing this operation on the entire dataset, let's first examine how this would look for the subset of the 90011 zip code. First let's look at the top 5 most frequent streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_freq_zip_subset.STREET.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's choose the highest occurring street."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_street_90011 = top_freq_zip_subset.STREET.value_counts().head(1).index[0]\n",
    "top_street_90011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will now be to perform this operation for all 63 zip codes! Let's quickly reinspect our df first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = la_data.groupby('ZIP_CD').apply(lambda df: df.STREET.value_counts().head(1).index[0])\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = grouped_data.reset_index()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = grouped_data.rename(columns = {0:'Top Street'})\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the top streets\n",
    "top_streets = list(grouped_data['Top Street'])\n",
    "top_streets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get latitude and longitude values for our zip codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_lat_lon = la_data.groupby('ZIP_CD').agg({'LAT': 'mean','LON': 'mean'})\n",
    "grouped_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the longitudes and latitudes\n",
    "latitudes = list(grouped_lat_lon['LAT'])\n",
    "longitudes = list(grouped_lat_lon['LON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_subset = pd.DataFrame(list(zip(zip_list,neigh_list,top_streets,latitudes,longitudes)), \n",
    "                         columns=['Zip Code','Neighborhood','Street','Latitude','Longitude'])\n",
    "la_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's remove all rows that don't have a neighborhood in them since these won't be helpful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_subset.dropna(subset=[\"Neighborhood\"], axis=0, inplace=True)\n",
    "la_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 rows were removed. Now let's proceed.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the neighborhoods in LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use geopy library to get the latitude and longitude values of LA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent <em>la_explorer</em>, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Los Angeles, California'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"la_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinates of LA are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a map of LA with the neighborhoods superimposed on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of LA using latitude and longitude values\n",
    "map_la = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, street, neighborhood in zip(la_subset['Latitude'], la_subset['Longitude'], la_subset['Street'], la_subset['Neighborhood']):\n",
    "    label = '{}, {}'.format(street, neighborhood)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_la)  \n",
    "    \n",
    "map_la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to start utilizing the Foursquare API to explore the neighborhoods and segment them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'VNYEHCLUY3WPOTA0AGKBMP1GBZU2IZV1TLLI3WKKUD5SH0FP' # your Foursquare ID\n",
    "CLIENT_SECRET = input('Enter your Foursquare Client Secret: ')\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "print(\"Moving on to the next step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore the first neighborhood in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_subset.loc[0,'Neighborhood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_latitude = la_subset.loc[0, 'Latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = la_subset.loc[0, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "neighborhood_name = la_subset.loc[0, 'Neighborhood'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get the top 100 venues that are in Los Angeles within a radius of 500 meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the GET request URL. The URL will be called **url**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100 # limit of number of venues returned by Foursquare API\n",
    "radius = 500 # define radius\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the GET request and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to clean the json and structure it into a *pandas* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many venues were returned by Foursquare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a function to repeat the same process to for all the neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now write code to run the above function on each neighborhood and create a new dataframe called *la_venues*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_venues = getNearbyVenues(names=la_subset['Neighborhood'],\n",
    "                                   latitudes=la_subset['Latitude'],\n",
    "                                   longitudes=la_subset['Longitude']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the size of the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la_venues.shape)\n",
    "la_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many venues were returned for each neighborhood/set of neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_venues.groupby('Neighborhood').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find out how many unique categories can be curated from all the returned venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(la_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the neighborhoods in LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to use our clustering algorithm, any categorical variable we would like to consider in our model will have to be transformed into a binary format. To achieve this, we will use the pandas method **get_dummies()** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "la_onehot = pd.get_dummies(la_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "la_onehot['Neighborhood'] = la_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [la_onehot.columns[-1]] + list(la_onehot.columns[:-1])\n",
    "la_onehot = la_onehot[fixed_columns]\n",
    "\n",
    "la_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's examine the new dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's group rows by neighborhoods and by taking the mean of the frequency of occurrence of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grouped = la_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "la_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's confirm the new size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print the neighborhoods along with the top 5 most common venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in la_grouped['Neighborhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = la_grouped[la_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's put that into a *pandas* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to sort the venues in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the new dataframe and display the top 10 venues for the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = la_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(la_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(la_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering the neighborhoods in LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clustering algorithm of choice here will be the [KMeans](https://en.wikipedia.org/wiki/K-means_clustering). It is considered to be one of the simplest clustering models. However, despite its simplicity, *k*-means is vastly used for clustering in many data science applications and is especially useful if you need to quickly discover insights from unlabeled data.\n",
    "\n",
    "The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the **inertia or within-cluster sum-of-squares**. \n",
    "\n",
    "A drawback to this algorithm is that it requires the number of clusters to be pre-specified. Therefore we will explore different numbers of k to try and find a good fit to our data.\n",
    "\n",
    "The evaluation criteria considered here will be the elbow method. The basic premise behind this method is that first of all we plot the sum of the differences between each point in a cluster and the associated cluster center. If the plot resembles an arm, then the **elbow** (the point of inflection on the curve) is a good indication that the underlying model fits best at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_grouped_clustering = la_grouped.drop('Neighborhood', 1)\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,11)\n",
    "\n",
    "for kclusters in K:\n",
    "    kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(la_grouped_clustering)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the choice for the optimal k is not so straightforward as even though k = 3 seems to be the elbow point, the sum of squared distances is quite high there. A choice between k=4 and k=7 seems viable, so let's go with k = 5 and run *k*-means to cluster the neighborhoods into 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(la_grouped_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "la_merged = la_subset\n",
    "\n",
    "# merge la_grouped with la_subset to add latitude/longitude for each neighborhood\n",
    "la_merged = la_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "la_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the cluster labels to be of type int and not contain missing values i.e. NaN values for the further analysis to work so let's quickly it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged['Cluster Labels'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are type int so that's okay. Let's check for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged['Cluster Labels'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are none! **Finally, let's visualize the resulting clusters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in (range(kclusters))]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(la_merged['Latitude'], la_merged['Longitude'], la_merged['Neighborhood'], la_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examining the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now examine each cluster and discover the distinctive venue categories that define each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 0, la_merged.columns[[1] + list(range(5, la_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 1, la_merged.columns[[1] + list(range(5, la_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 2, la_merged.columns[[1] + list(range(5, la_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 3, la_merged.columns[[1] + list(range(5, la_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_merged.loc[la_merged['Cluster Labels'] == 4, la_merged.columns[[1] + list(range(5, la_merged.shape[1]))]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
